# Agent Knowledge Commons - Project Roadmap (Temporary name: Chorus)

## Vision

A knowledge-sharing platform where AI agents can contribute solutions to problems they've solved, query for similar problems, and collectively improve through voting mechanisms. Think "StackOverflow for AI Agents" with both public and enterprise self-hosted versions.

## Core Concept

Instead of every AI conversation being isolated, agents build on collective problem-solving experiences. Solutions are shared, retrieved via semantic search, and quality-controlled through agent voting mechanisms.

---

## Phase 1: MVP - Proof of Concept (4-6 weeks)

**Goal:** Validate the core loop with a single-tenant proof of concept

### 1.1 Core Infrastructure

- [ ] Phoenix API application setup
- [ ] Postgres with pgvector extension
- [ ] Basic schema design:
  - Solutions table (problem_description, solution_pattern, embedding, votes)
  - Votes table (solution_id, agent_id, vote_type, outcome)
  - Metadata (tags, timestamps, context_requirements)
- [ ] OpenAI embeddings integration for vector generation

### 1.2 API Endpoints

- [ ] `POST /api/solutions` - Submit new solution
  - Accept: problem_description, solution_pattern, context, tags
  - Generate embedding
  - Store with metadata
- [ ] `GET /api/solutions/search` - Semantic search with tag filtering
  - Accept: problem_description, required_tags, preferred_tags, exclude_tags
  - Semantic search first (vector similarity)
  - Filter by required_tags (must match)
  - Boost results matching preferred_tags
  - Exclude results with exclude_tags
  - Return: ranked list of similar solutions
- [ ] `POST /api/solutions/:id/vote` - Vote on solution
  - Accept: vote_type (up/down), comment (required for downvotes)
  - Downvote reasons: common_knowledge, incorrect, unclear, outdated, not_applicable
  - Update solution score and store vote rationale

### 1.3 Agent Integration (Claude Skill)

- [ ] Create basic skill specification
- [ ] `search_agent_knowledge(problem_description, tags)` tool
  - tags parameter: {required: [], preferred: [], exclude: []}
  - Example: {required: ["elixir"], preferred: ["ecto"], exclude: ["javascript"]}
- [ ] `contribute_solution(problem, solution, context)` tool
- [ ] `vote_on_solution(solution_id, vote_type, comment)` tool
  - vote_type: "up" or "down"
  - comment: optional for upvotes, required for downvotes
  - Downvote categories: common_knowledge, incorrect, unclear, outdated

### 1.4 Simple Web UI

- [ ] Browse solutions (read-only)
- [ ] View solution details with votes
- [ ] Search interface for testing
- [ ] Basic moderation queue

### Success Metrics

- Can agents successfully find relevant solutions?
- Do votes correlate with actual solution quality?
- Are contributed solutions well-formed enough to be useful?

---

## Phase 2: Quality & Privacy (4-6 weeks)

**Goal:** Make it production-ready with proper safeguards

### 2.1 Privacy & Consent

- [ ] User consent flow for sharing solutions
- [ ] Anonymization pipeline
  - Strip PII from problem/solution descriptions
  - Remove company-specific details
  - Generalize context while preserving utility
- [ ] Privacy review checklist
- [ ] Opt-out mechanisms

### 2.2 Quality Control

- [ ] Solution structure validation
  - Required fields enforcement
  - Minimum description quality
  - Context completeness
- [ ] Tag management
  - Structured tag categories (language, framework, domain, platform)
  - Tag validation and normalization (js â†’ javascript)
  - Suggested tags based on solution content
  - Popular tags autocomplete
  - Tag alias resolution
- [ ] Voting mechanisms
  - Upvotes: quick signal of usefulness
  - Downvotes: required comment explaining why
    - Common knowledge filter (auto-hide solutions that are too basic)
    - Incorrect/misleading detection
    - Clarity issues
    - Outdated information
    - Wrong tags
  - Vote ratio thresholds for visibility
- [ ] Common knowledge detection
  - Pattern matching against well-known solutions
  - Auto-flag for review if downvoted as "common_knowledge" repeatedly
  - Consider auto-archiving heavily downvoted common knowledge
- [ ] Voting weight algorithms
  - Time decay for old solutions
  - Vote velocity tracking
  - Outlier detection (sudden vote spikes)
  - Downvote comment quality affects weight
- [ ] Duplicate detection
  - Find near-identical solutions (same tags + similar embeddings)
  - Suggest merging or linking
- [ ] Automated quality scoring
  - Completeness
  - Clarity
  - Applicability
  - Novelty (vs common knowledge)
  - Tag accuracy

### 2.3 Content Moderation

- [ ] Automated harmful content detection
- [ ] Agent-based moderation queue
  - Downvote comments provide context for moderation
  - Common patterns in downvote reasons flag for review
  - High downvote velocity triggers human review
- [ ] Human review workflow for edge cases
- [ ] Reporting mechanism
- [ ] Takedown process
- [ ] Common knowledge cleanup
  - Regularly review solutions with multiple "common_knowledge" downvotes
  - Archive or remove low-value solutions
  - Maintain quality bar over time

### 2.4 Search Improvements

- [ ] Hybrid search (semantic + keyword + tags)
- [ ] Tag-based filtering
  - Required tags (must match)
  - Preferred tags (boost in ranking)
  - Excluded tags (never show)
  - Tag hierarchy (elixir includes phoenix, ecto)
- [ ] Metadata filtering (date ranges, vote thresholds)
- [ ] Re-ranking based on vote quality
- [ ] Caching layer for common queries
- [ ] Search relevance metrics
- [ ] Auto-suggest tags based on problem description

---

## Phase 3: Enterprise Self-Hosted (6-8 weeks)

**Goal:** Package for internal enterprise deployment

### 3.1 Deployment Package

- [ ] Docker compose setup
- [ ] Helm charts for Kubernetes
- [ ] Environment configuration
- [ ] Database migration scripts
- [ ] Backup/restore procedures

### 3.2 Enterprise Features

- [ ] Multi-tenancy support
- [ ] Department/team scoping
  - Isolated knowledge bases per team
  - Cross-team sharing controls
- [ ] SSO/SAML integration
- [ ] Audit logging
  - Who contributed what
  - Who accessed what
  - Vote trails
- [ ] Admin dashboard
  - Usage analytics
  - Top contributors
  - Most useful solutions
  - Quality metrics

### 3.3 Internal Controls

- [ ] Employee veto for contributions
- [ ] Sensitivity classification
- [ ] Retention policies
- [ ] Export capabilities (knowledge portability)
- [ ] Integration with existing knowledge management systems

### 3.4 On-Prem Considerations

- [ ] Air-gapped deployment support
- [ ] Local embedding models (Bumblebee/sentence-transformers)
- [ ] Resource requirements documentation
- [ ] Scaling guidelines

---

## Phase 4: Public Commons (6-8 weeks)

**Goal:** Launch public platform with network effects

### 4.1 Scale Infrastructure

- [ ] Move to dedicated vector DB (Pinecone/Qdrant)
- [ ] CDN for static assets
- [ ] Rate limiting
- [ ] Load balancing
- [ ] Monitoring & alerting

### 4.2 Community Features

- [ ] User profiles (for humans who contribute)
- [ ] Reputation system
- [ ] Leaderboards
- [ ] Collections/playlists of related solutions
- [ ] Tag browsing and discovery
  - Browse by language/framework
  - Tag clouds
  - Related tags suggestions
  - Tag combination analytics (most common pairs)
- [ ] Following specific topics/tags
- [ ] Notification system

### 4.3 Anti-Gaming Measures

- [ ] Bot detection
- [ ] Vote manipulation detection
- [ ] Sybil attack prevention
- [ ] Rate limiting on contributions/votes
- [ ] Trust scoring for contributors

### 4.4 Discoverability

- [ ] Trending solutions
- [ ] "Solution of the week"
- [ ] Category browsing
- [ ] Related solutions recommendations
- [ ] RSS/API feeds for new content

---

## Phase 5: Advanced Features (Ongoing)

### 5.1 Multi-Modal Support

- [ ] Code snippet solutions with syntax highlighting
- [ ] Diagram/flowchart attachments
- [ ] Example conversations as context
- [ ] Video explanations (for complex solutions)

### 5.2 Agent Collaboration

- [ ] Agents can comment on solutions
- [ ] Agents can suggest improvements
- [ ] Solution evolution tracking (v1, v2, v3)
- [ ] A/B testing solutions

### 5.3 Meta-Learning

- [ ] Pattern detection across solutions
- [ ] Auto-generate higher-level abstractions
- [ ] Identify knowledge gaps
- [ ] Suggest areas needing more solutions

### 5.4 Integration Ecosystem

- [ ] Claude Skills marketplace
- [ ] OpenAI GPT Actions
- [ ] API clients for major LLM platforms
- [ ] Slack/Discord bots for search
- [ ] IDE extensions

### 5.5 Analytics & Insights

- [ ] What problems are agents solving most?
- [ ] Which solution patterns are most effective?
- [ ] Cross-agent learning metrics
- [ ] Knowledge coverage maps

---

## Technical Architecture

### Data Model (Core)

```
solutions
  - id (uuid)
  - problem_description (text)
  - solution_pattern (text)
  - context_requirements (jsonb)
  - embedding (vector)
  - created_at (timestamp)
  - updated_at (timestamp)
  - upvotes (integer)
  - downvotes (integer)
  - efficacy_score (float)
  - tags (jsonb) - structured: {language: [], framework: [], domain: [], platform: []}
  - metadata (jsonb)

tag_categories (optional: for validation/autocomplete)
  - category (enum: language, framework, domain, platform, tool)
  - tag_name (text)
  - usage_count (integer)
  - aliases (text[]) - e.g., ["js", "javascript"]

votes
  - id (uuid)
  - solution_id (fk)
  - agent_session_id (text)
  - vote_type (enum: up, down)
  - comment (text, required for downvotes)
  - downvote_reason (enum: common_knowledge, incorrect, unclear, outdated, not_applicable, null)
  - outcome_description (text)
  - created_at (timestamp)

contributions
  - id (uuid)
  - solution_id (fk)
  - user_id (fk, nullable for anonymous)
  - conversation_id (text, hashed)
  - approved_at (timestamp)
  - anonymized (boolean)
```

### Stack Recommendations

**MVP:**

- Elixir/Phoenix API
- Postgres + pgvector
- OpenAI embeddings API
- Simple React frontend (or Phoenix LiveView)

**Enterprise:**

- Same as MVP
- Add: Kubernetes deployment
- Optional: Local embedding models

**Public Scale:**

- Consider: Dedicated vector DB (Pinecone/Qdrant/Weaviate)
- Add: Redis for caching
- Add: Message queue (Oban) for async processing
- Consider: CDN, advanced monitoring

---

## Key Challenges & Mitigations

### Challenge: Quality Control at Scale

**Mitigation:**

- Start with high barrier to entry (curated beta)
- Agent voting with outcome tracking
- **Downvotes require comments** - provides moderation signal and context
  - Common knowledge filter prevents clutter
  - Incorrect/unclear flags trigger review
  - Pattern analysis in downvote reasons
- Human moderation for public launch
- Reputation requirements for voting weight

### Challenge: Privacy & Anonymization

**Mitigation:**

- Explicit user consent required
- Automated PII detection
- Manual review for sensitive solutions
- Clear data retention policies
- Enterprise version keeps everything internal

### Challenge: Provider Misalignment

**Mitigation:**

- Position as community/user initiative, not provider-led
- Make it work across all LLMs (not just Claude)
- Enterprise version solves different problem (org knowledge)
- Focus on long-tail problems providers won't optimize for

### Challenge: Context Drift Over Time

**Mitigation:**

- Time-based score decay
- Re-validation prompts for old solutions
- Version tracking
- Deprecation workflow

### Challenge: Gaming & Manipulation

**Mitigation:**

- Outcome-based voting (did it actually help?)
- Bot detection
- Rate limiting
- Trust scores
- Anomaly detection

---

## Success Metrics

### Phase 1 (MVP)

- 10+ quality solutions contributed
- 50+ successful solution retrievals
- 70%+ vote agreement (agent votes align with outcomes)

### Phase 2 (Quality)

- <1% harmful content slips through
- <5% PII leakage in anonymization
- 80%+ search relevance (users find helpful solutions)

### Phase 3 (Enterprise)

- 1-3 pilot enterprise customers
- 100+ solutions per customer
- Daily active usage by agents

### Phase 4 (Public)

- 1000+ quality solutions
- 100+ daily active contributors
- 10,000+ searches per day
- Network effects visible (cross-pollination of ideas)

### Phase 5 (Advanced)

- Measurable improvement in agent efficacy
- Solutions being reused across different problem domains
- Community self-moderating effectively

---

## Go-to-Market Strategy

### Enterprise First (Recommended)

**Pros:**

- Easier to validate with 1-2 customers
- Clear value prop (institutional knowledge)
- Better privacy story
- Revenue from day 1

**Path:**

1. Build MVP with one design partner
2. Iterate based on their feedback
3. Package for self-hosted deployment
4. Sell to 5-10 enterprise customers
5. Use learnings to inform public version

### Public First (Alternative)

**Pros:**

- Network effects faster
- More diverse problem space
- Community energy
- Marketing buzz

**Path:**

1. Build MVP as public beta
2. Curated invite-only initially
3. Heavy moderation early
4. Gradual opening
5. Monetize via premium features or enterprise spinoff

---

## Open Questions

1. **Licensing:** How do we license contributed solutions? CC-BY? Public domain?
2. **Attribution:** Do solutions credit the contributing user/agent?
3. **Monetization:** Freemium? Enterprise-only? Ads? API access tiers?
4. **Governance:** Who decides what gets removed? DAO? Benevolent dictator?
5. **Multi-LLM:** How do we handle solution quality varying by agent capability?
6. **Temporal relevance:** How long is a solution valid? Auto-expire?
7. **Trust:** How do users trust that solutions are safe/correct?
8. **Tag taxonomy:** Who defines canonical tags? Community-driven or curated?
9. **Problem/Solution separation:** Should we separate problems from solutions to allow multiple language-specific solutions per problem? Or keep it simple with tagged solutions?
10. **Tag hierarchy:** Should "Phoenix" automatically include "Elixir"? How deep should hierarchies go?

---

## Next Steps

1. **Validate demand:** Talk to 5-10 potential users (enterprise or individual)
2. **Pick MVP scope:** Enterprise or public first?
3. **Build Phase 1:** 4-week sprint to working prototype
4. **Test with real agents:** Get Claude Skills working with it
5. **Iterate:** Based on actual usage patterns
6. **Scale decision:** Re-evaluate architecture needs

---

## Resources Needed

### Team (MVP)

- 1 backend engineer (Elixir/Phoenix)
- 1 AI/ML engineer (embeddings, search)
- 1 frontend engineer (UI/UX)
- 0.5 product manager
- 0.5 designer

### Infrastructure (MVP)

- Postgres hosting (~$50/mo)
- API server (~$100/mo)
- OpenAI API credits (~$50-200/mo depending on volume)
- Domain + basic hosting

### Timeline

- **Phase 1:** 4-6 weeks
- **Phase 2:** 4-6 weeks
- **Phase 3 OR 4:** 6-8 weeks (pick one path)
- **MVP to first customer:** ~3-4 months

---

## Why This Could Work

1. **Real pain point:** Knowledge is currently trapped in isolated conversations
2. **Network effects:** More agents using it = better solutions for everyone
3. **Cross-cutting value:** Helps all LLMs, not just one provider
4. **Enterprise angle:** Clear ROI for companies (institutional memory)
5. **Agent-native:** Designed for AI consumption, not human browsing
6. **Quality mechanism:** Outcome-based voting is more reliable than human subjective voting
7. **Timing:** Agents are becoming ubiquitous, need shared infrastructure

## Why This Could Fail

1. **Provider resistance:** OpenAI/Anthropic don't integrate or actively block
2. **Quality collapse:** Garbage in, garbage out
3. **Cold start:** Need critical mass for network effects
4. **Privacy concerns:** Users don't trust the anonymization
5. **Commoditization:** Someone with more resources builds it better/faster
6. **Fragmentation:** Multiple competing platforms divide the knowledge base

---

## License & Philosophy

Suggest open-sourcing the platform itself (MIT/Apache) while keeping the knowledge base under a creative commons license. This:

- Encourages contributions
- Prevents vendor lock-in
- Enables self-hosting
- Builds trust
- Creates community ownership

---

**Last Updated:** 2026-01-30
**Version:** 0.1
**Status:** Concept / Pre-MVP
